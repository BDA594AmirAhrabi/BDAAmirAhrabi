{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DeYXbqK-rRW"
      },
      "source": [
        "## Classifying text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MejilF82-rRZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV as gs\n",
        "import sklearn.feature_extraction.text as text\n",
        "import sklearn.naive_bayes as nb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IFIKbfRhfz_"
      },
      "source": [
        "We turn to applying machine learning classification methods to text. There are\n",
        "no new principles at stake.  In principle, everything is the same as it was for\n",
        "learning how to classify irises.\n",
        "\n",
        "1.  We need to find labeled data; each of the exemplars in the data should be represented with a fixed set of features.  \n",
        "2. We need to split our data and training and test data.  \n",
        "3. We need to train learner on the training data and evaluate it (test it) it on the test data.\n",
        "\n",
        "The problem is that text data is not in a form  that is compatible with\n",
        "what we have learned about classifiers.  The text must be put in a suitable\n",
        "form before a linear model; can be trained on it.\n",
        "\n",
        "**Training**\n",
        "\n",
        "1.  Labeled data must be loaded (into Python).  It should be a sequence of documents T accompanied by a sequence of labels L.\n",
        "2.  Split T and L into training and test groups, yielding T1 and T2; as well as and L1 and L2.\n",
        "2.  Train or a **feature model** on the training data T1 (or in scikit learn terminology **fit** the model **to** the training data).  The feature model inputs the text sequence and outputs a **term-document** matrix suitable for training a linear classifier.  The feature model is called a **vectorizer**\n",
        "(because it turns a document into a vector, a column of numbers).\n",
        "3.  Using the trained vectorizer, transform T1 into a term document matrix M1.\n",
        "4.  Train a linear model $\\mu$ on M1 and L1.\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "1.  Transform the test data T2 into a term document matrix M2 using the vectorizer fit during step 2 of training;  in particular this means if there are words in the T2 data that were never seen during training, they are ignored in building M2.\n",
        "2.  Use $\\mu$  to classify the texts represented in M2; that is produce a set of predicted labels P2.\n",
        "3.  Compare the actual labels L2 with the predicted labels P2 using standard evaluation metrics such as precision, accuracy, and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qildTjvw-rRb"
      },
      "source": [
        "## Review the steps with insult detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28jZSTW_-rRb"
      },
      "source": [
        "We looked at the insult detection data in  the text classification notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONoGGdK8hfz_"
      },
      "source": [
        "### Training step 1: Loading the data\n",
        "\n",
        "Let's load the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MtGQlB1q-rRc"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
        "'text_classification/'\n",
        "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
        "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ncLUYx-rRc"
      },
      "source": [
        "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the date, and the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "pFeNaW-m-rRd",
        "outputId": "18ac18e1-814a-4dc2-e10f-87ae94ad6149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Insult             Date  \\\n",
              "3942       1  20120502172717Z   \n",
              "3943       0  20120528164814Z   \n",
              "3944       0  20120620142813Z   \n",
              "3945       0  20120528205648Z   \n",
              "3946       0  20120515200734Z   \n",
              "\n",
              "                                                Comment  \n",
              "3942  \"you are both morons and that is never happening\"  \n",
              "3943  \"Many toolbars include spell check, like Yahoo...  \n",
              "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
              "3945  \"How about Felix? He is sure turning into one ...  \n",
              "3946  \"You're all upset, defending this hipster band...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce34d1a5-b07c-48bd-b782-1fc057e685a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Date</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3942</th>\n",
              "      <td>1</td>\n",
              "      <td>20120502172717Z</td>\n",
              "      <td>\"you are both morons and that is never happening\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3943</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528164814Z</td>\n",
              "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3944</th>\n",
              "      <td>0</td>\n",
              "      <td>20120620142813Z</td>\n",
              "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3945</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528205648Z</td>\n",
              "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3946</th>\n",
              "      <td>0</td>\n",
              "      <td>20120515200734Z</td>\n",
              "      <td>\"You're all upset, defending this hipster band...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce34d1a5-b07c-48bd-b782-1fc057e685a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce34d1a5-b07c-48bd-b782-1fc057e685a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce34d1a5-b07c-48bd-b782-1fc057e685a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9d022973-702d-4872-925b-1615144f446d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d022973-702d-4872-925b-1615144f446d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9d022973-702d-4872-925b-1615144f446d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibxc3U3K-rRh"
      },
      "source": [
        "Now we define the text sequences $\\mathbf{T}$ and the label sequence  $\\mathbf{L}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wty7pj42-rRh"
      },
      "outputs": [],
      "source": [
        "T = df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-8xJstOo-rRi"
      },
      "outputs": [],
      "source": [
        "L = df['Insult']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ziFnk3hf0A"
      },
      "source": [
        "### Step 2 Split the data and labels into training and test groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BtmqKDN-hf0A"
      },
      "outputs": [],
      "source": [
        "T1, T2, L1, L2 = train_test_split(T,L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1YnKgklhf0A"
      },
      "source": [
        "### Step 3 and 4:  Fit the feature model (vectorizer) to the training data and Transform  it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "H-fPVjYV-rRl"
      },
      "outputs": [],
      "source": [
        "tf = text.TfidfVectorizer()\n",
        "# Scikit learn has one function that does both fitting and transforming.\n",
        "# M1 is the transformed data\n",
        "# tf is the trained feature model (which will be used to transform the test data)\n",
        "M1 = tf.fit_transform(T1)#.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0wp6RbS-rRo"
      },
      "source": [
        "### Step 5 Training the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88mG_1C0-rRo"
      },
      "source": [
        "Now, we are going to train a classifier as usual. We first split the data into a train and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf-3XQDN-rRo"
      },
      "source": [
        "We use a **Bernoulli Naive Bayes classifier**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uj7vvTS--rRo"
      },
      "outputs": [],
      "source": [
        "# Create classifer\n",
        "bnb =nb.BernoulliNB()\n",
        "#bnb= nb.MultinomialNB()\n",
        "#bnb =nb.GaussianNB()\n",
        "\n",
        "# Fit (train) the classifier  using the training data and labels\n",
        "bnb.fit(M1, L1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nb3-WWqhf0B"
      },
      "source": [
        "This function collects what we've down so far, plus it produces predictions for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3qIIizr7hf0B"
      },
      "outputs": [],
      "source": [
        "def split_vectorize_and_fit(docs,labels,clf,**params):\n",
        "    \"\"\"\n",
        "    Given labeled data (docs, labels) and a classifier,\n",
        "    do the training test split.  Train the vectorizer and the classifier.\n",
        "    Transform the test data and return a set of preducted labels\n",
        "    for the test data,\n",
        "    \"\"\"\n",
        "    T_train,T_test, y_train,y_test = train_test_split(docs,labels)\n",
        "    tf = text.TfidfVectorizer(**params)\n",
        "    X_train = tf.fit_transform(T_train)\n",
        "    clf_inst = clf()\n",
        "    clf_inst.fit(X_train, y_train)\n",
        "    X_test = tf.transform(T_test)\n",
        "    return clf_inst.predict(X_test), y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8rc_wMYhf0B"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvhBRFiIhf0B"
      },
      "source": [
        "Evaluate the classifier, first using accuracy (what `.score()` returns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewbP2vT-rRp",
        "outputId": "e9d094e1-5fa1-4953-c890-b2e2f2208450"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7750759878419453"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# vectorize the test data using the vectorizer trained on T1\n",
        "# Notice we DONT call .fit_transform() because that would retrain the vectorizer on the test data\n",
        "# We call .transform() using the trained model to transform the new data.\n",
        "# Words not seen during training will be ignored.\n",
        "M2 = tf.transform(T2)#.toarray()\n",
        "# Classify the data using the trained classisifer and report the accuracy\n",
        "bnb.score(M2, L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38bf79W-rRp"
      },
      "source": [
        "Now try re-executing steps 2 through 5.  (Just re-execute the cells)  The results should be the same, right?\n",
        "\n",
        "Well, are they?  \n",
        "\n",
        "What happens:  each training test split produces a different set of test data.  Sometimes the test is harder.\n",
        "Sometimes it's easier.  Or looking at it another way:  Sometimes the training data is a better preparation for the test than others.  \n",
        "\n",
        "To get a realistic view of how our classifier is doing we take the average performance on a  number of\n",
        "train/test splits.  This is called **cross validation**.  We return to that point below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSBZ-uUJhf0B"
      },
      "source": [
        "#### Using all three evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_8qgUbphf0B"
      },
      "source": [
        "First let's get more evaluation numbers, in particular precision and recall.  We do\n",
        "that by calling a method that returns the predicted labels P2, so we can compare\n",
        "L2 and P2 using different evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmi_Rsslhf0B",
        "outputId": "4af03644-db4e-4c89-e9f9-99bb4d4f682d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78 Precision: 0.14 Recall: 0.95\n"
          ]
        }
      ],
      "source": [
        "P2 = bnb.predict(M2)\n",
        "scores = np.array([accuracy_score(P2, L2),\n",
        "                   precision_score(P2, L2),\n",
        "                   recall_score(P2, L2)])\n",
        "print(f'Accuracy: {scores[0]:.2f} Precision: {scores[1]:.2f} Recall: {scores[2]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhBR8GxKhf0B"
      },
      "source": [
        "We see that the accuracy is a bit misleading.  There is a serious precision problem.\n",
        "\n",
        "What does that mean in the setting of insult detection?  It means the BNB classifier is a little too\n",
        "eager to call something an insult.  When it flags something as an insult, it\n",
        "is right only 14% of the time.\n",
        "\n",
        "Why would that be?  Think about how the model is trained and what its weakness might be.\n",
        "This is what it means to try to interpret or discuss a model's performance.  Zoom\n",
        "in the model's weakness. Talk about where that weakness comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bt36GeMpFW"
      },
      "source": [
        "#### Basic train and test loop\n",
        "\n",
        "See the Insults with Naive Bayes Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zaE0KDK5-rRr"
      },
      "source": [
        "## Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg50G61U-rRr"
      },
      "source": [
        "Work through the Insults Detection notebook about text classification and\n",
        "insult detection. Focus on the use of `scikit_learn`, especially the\n",
        "`TfidfVectorizer`. For this assignment you will be turning in the Python notebook (extension `.ipynb`, **not** a `.py` file).  Turn in this notebook with all the code needed to run your classifier.  If it doesn't run, your score will suffer.\n",
        "\n",
        "For Parts One and Two try two different classifiers on the movie review data, the one used in the textbook, an SVM called `LinearSVC`, and  the Bernoulli Naive Bayes model used above. Some points of emphasis;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zh0oy2shf0C"
      },
      "source": [
        "#### PART ONE\n",
        "\n",
        "1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.  2 points\n",
        "2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_vectorize_and_fit` defined above useful, but you will need to modify it.  2 points.\n",
        "3.  Discuss which of the two classifiers does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting). 3 points.\n",
        "4. Do a new training/test split on the data and train and test an SVM model.  Choose one false positive and one false negative from the test set.  Call these documents $j$ and $k$ and call their functional margins $c_j$ and $c_k$ (see the SVM notebook).  Find\n",
        "\n",
        "$$\n",
        "\\frac{c_{j}}{c_{max}-c_{min}}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\frac{c_{k}}{c_{max}-c_{min}},\n",
        "$$\n",
        "\n",
        "where $c_{max}$ and $c_{min}$ are the maximum and minimum functional margins for the training set.  Are documents $j$ and $k$ misclassified with high confidence?  Of course getting credit for this part means submitting the code you used to compute these quantities.  For the computation of functional margins, it will be convenient to relabel positive and negative classes 1 and -1 respectively. 5 points"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "def get_file_strings(corpus, file_ids):\n",
        "    return [corpus.raw(file_id) for file_id in file_ids]\n",
        "\n",
        "pos_reviews = get_file_strings(mr, mr.fileids('pos'))\n",
        "neg_reviews = get_file_strings(mr, mr.fileids('neg'))\n",
        "\n",
        "data = [(review, 1) for review in pos_reviews] + [(review, 0) for review in neg_reviews]\n",
        "shuffle(data)\n",
        "texts, labels = zip(*data)\n",
        "\n",
        "def split_vectorize_and_fit(texts, labels, classifier):\n",
        "    accuracies, precisions, recalls = [], [], []\n",
        "    for _ in range(10):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=None)\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "        X_test_vec = vectorizer.transform(X_test)\n",
        "        clf = classifier()\n",
        "        clf.fit(X_train_vec, y_train)\n",
        "        y_pred = clf.predict(X_test_vec)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        precisions.append(precision_score(y_test, y_pred))\n",
        "        recalls.append(recall_score(y_test, y_pred))\n",
        "    return np.mean(accuracies), np.mean(precisions), np.mean(recalls)\n",
        "\n",
        "svc_metrics = split_vectorize_and_fit(texts, labels, LinearSVC)\n",
        "nb_metrics = split_vectorize_and_fit(texts, labels, BernoulliNB)\n",
        "\n",
        "print(\"LinearSVC Metrics: Accuracy, Precision, Recall\")\n",
        "print(svc_metrics)\n",
        "print(\"BernoulliNB Metrics: Accuracy, Precision, Recall\")\n",
        "print(nb_metrics)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def calculate_functional_margins(texts, labels):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    svm = LinearSVC()\n",
        "    svm.fit(X_train_vec, y_train)\n",
        "    y_pred = svm.predict(X_test_vec)\n",
        "    false_pos = [i for i, (y, pred) in enumerate(zip(y_test, y_pred)) if y == 0 and pred == 1]\n",
        "    false_neg = [i for i, (y, pred) in enumerate(zip(y_test, y_pred)) if y == 1 and pred == 0]\n",
        "    if false_pos and false_neg:\n",
        "        fp_idx = false_pos[0]\n",
        "        fn_idx = false_neg[0]\n",
        "        margins = svm.decision_function(X_test_vec)\n",
        "        c_max = np.max(margins)\n",
        "        c_min = np.min(margins)\n",
        "        c_fp = margins[fp_idx]\n",
        "        c_fn = margins[fn_idx]\n",
        "        print(f\"False Positive Margin: {(c_fp - c_min) / (c_max - c_min)}\")\n",
        "        print(f\"False Negative Margin: {(c_fn - c_min) / (c_max - c_min)}\")\n",
        "    else:\n",
        "        print(\"No false positives or negatives found in this split.\")\n",
        "\n",
        "calculate_functional_margins(texts, labels)"
      ],
      "metadata": {
        "id": "ml3DNhHmkx50",
        "outputId": "aa800ee1-ce9a-4dd0-f212-2ed4bee5278d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC Metrics: Accuracy, Precision, Recall\n",
            "(0.8355, 0.8207656497460223, 0.8477206964492631)\n",
            "BernoulliNB Metrics: Accuracy, Precision, Recall\n",
            "(0.7842500000000001, 0.8774438442263344, 0.6791984696886508)\n",
            "False Positive Margin: 0.5066834761051844\n",
            "False Negative Margin: 0.35242522331354037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQtgUsuDhf0C"
      },
      "source": [
        "#### PART TWO\n",
        "\n",
        "5. Using the SVM classifier and training on **all** the data find the 50 most important Positive features for the Movie Reviews Data.  They should differ significantly from the most important features in Insult Detection. The function `print_topn` (from the Insult Detection Notebook) should be of help. 4 Points\n",
        "6.  Find the 100 most important Negative features for the Movie Reviews Data. Note that the way two-class problems work with SVMs there is only one set of weights to look at, so it won't work to pass more than one class name to the `class_labels` parameter of `print_top_n` (it would work with a NaiveBayes classifier).  In particular: you need the **lowest** weighted features if you want to look at the more fun word set that best characterized bad reviews. You will have to modify `print_topn` (from the Insult Detection Notebook) to do that. Try to do so in such a way that with one set of parameters it prints the most positive words, and with another, it prints the most negative words. You may notice the names of a few actors appearing in this feature set. Try not to laugh as the meaning of this dawns on you.  For an extra bit of approval from your instructor, while you're at it, modify it so that it returns a list of words in addition to printing them.  You should probably change the name of the function to `get_topn` if you succeed.  4 Points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wazBGWTsS4"
      },
      "source": [
        "#### Help with getting the movie reviews data.\n",
        "\n",
        "Execute the next two cells to get the movie review data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvlkaI1x-rRr",
        "outputId": "1b007735-cbde-4095-ea68-a405f9130ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8nQdcr4aH_dP"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews as mr\n",
        "\n",
        "def get_file_strings (corpus, file_ids):\n",
        "    return [corpus.raw(file_id) for file_id in file_ids]\n",
        "\n",
        "data = dict(pos = mr.fileids('pos'),\n",
        "            neg = mr.fileids('neg'))\n",
        "\n",
        "pos_file_ids = data['pos']\n",
        "neg_file_ids = data['neg']\n",
        "\n",
        "# Get all the positive and negative reviews.\n",
        "pos_file_reviews = get_file_strings (mr, pos_file_ids)\n",
        "neg_file_reviews = get_file_strings (mr, neg_file_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PY6_uHMJvqC"
      },
      "source": [
        "Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.\n",
        "You could code that up.\n",
        "\n",
        "What you'd really like to do is use `split_fit_and_eval`, defined above, which does a lot of the work for you.\n",
        "\n",
        "But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have\n",
        "one sequence of positive documents  and another sequence of negative documents.  \n",
        "\n",
        "So you will need to turn those two sequences into a sequence of documents and a sequence of labels\n",
        "because that's what `split_fit_and_eval` wants.  You also want the doc sequence\n",
        "to contain a random mixture of positive and negative documents, because some machine\n",
        "learning algorithms are sensitive to the order in which training data is presented to\n",
        "them.\n",
        "\n",
        "The next cell does **not** do that for you.  But it illustrates an approach using\n",
        "two sets of English letters in place of two sets of English documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqX53tNahf0D",
        "outputId": "a5434c1f-e8b8-495b-fa45-f98138380d5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 'b', 'c'), (1, 2, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "list(zip(*[('a',1),('b',2),('c',3)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCdgmoToRV8z",
        "outputId": "e9f902ff-19c0-410c-b0b1-2c584b4502eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklm\n",
            "nopqrstuvwxyz\n",
            "('v', 'l', 't', 'w', 'p', 'q', 'c', 'a', 'k', 'f', 'g', 'b', 'e', 'i', 'z', 'y', 'm', 'h', 'o', 'n', 'd', 'r', 's', 'j', 'u', 'x')\n",
            "('l', 'f', 'l', 'l', 'l', 'l', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'l', 'l', 'f', 'f', 'l', 'l', 'f', 'l', 'l', 'f', 'l', 'l')\n"
          ]
        }
      ],
      "source": [
        "# Lets work on letters instead of documents\n",
        "# There are 2 classes, letters from the first half of the\n",
        "# alphabet ('f') and letters frmm the last half ('l')\n",
        "\n",
        "from random import shuffle\n",
        "from string import ascii_lowercase\n",
        "\n",
        "#Class 1 of the letters: the f_lets\n",
        "f_lets = ascii_lowercase[:13]\n",
        "print(f_lets)\n",
        "#Class2 of the letters: the l_lets\n",
        "l_lets = ascii_lowercase[13:]\n",
        "print(l_lets)\n",
        "\n",
        "# Now get pairs of letters and labels\n",
        "f_pairs = [(let,'f') for let in f_lets]\n",
        "l_pairs = [(let,'l') for let in l_lets]\n",
        "\n",
        "###########  Shuffling  ###########################\n",
        "# Way too orderly, the classes arent mixed yet.\n",
        "data = f_pairs + l_pairs\n",
        "shuffle(data)\n",
        "###################  Now they're shuffled! ###############\n",
        "\n",
        "# Separate the letters from their labels\n",
        "lets, lbls = zip(*data)\n",
        "print(lets)\n",
        "print(lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z6fHq7Jhf0D",
        "outputId": "d3f96966-723e-406b-9924-418cc6e521cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3), ('a', 'b', 'c')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "list(zip(*[(1,\"a\"),(2,\"b\"),(3,\"c\")]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "def get_file_strings(corpus, file_ids):\n",
        "    return [corpus.raw(file_id) for file_id in file_ids]\n",
        "\n",
        "pos_reviews = get_file_strings(mr, mr.fileids('pos'))\n",
        "neg_reviews = get_file_strings(mr, mr.fileids('neg'))\n",
        "\n",
        "data = [(review, 1) for review in pos_reviews] + [(review, 0) for review in neg_reviews]\n",
        "shuffle(data)\n",
        "texts, labels = zip(*data)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_all = vectorizer.fit_transform(texts)\n",
        "\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_all, labels)\n",
        "\n",
        "def get_topn_features(vectorizer, classifier, n=50, positive=True):\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "    coefficients = classifier.coef_.flatten()  #access the coefficients\n",
        "    if positive:\n",
        "        top_indices = np.argsort(coefficients)[-n:]\n",
        "    else:\n",
        "        top_indices = np.argsort(coefficients)[:n]\n",
        "    top_features = feature_names[top_indices]\n",
        "    top_coefficients = coefficients[top_indices]\n",
        "    result = list(zip(top_features, top_coefficients))\n",
        "    print(f\"Top {'Positive' if positive else 'Negative'} Features:\")\n",
        "    for feature, coefficient in reversed(result):\n",
        "        print(f\"{feature}: {coefficient:.4f}\")\n",
        "    return result\n",
        "\n",
        "positive_features = get_topn_features(vectorizer, svm, n=50, positive=True)\n",
        "negative_features = get_topn_features(vectorizer, svm, n=100, positive=False)"
      ],
      "metadata": {
        "id": "RJi0sMHKlEP2",
        "outputId": "bd008d1e-a056-4e68-8067-ceda979554d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Positive Features:\n",
            "and: 1.9328\n",
            "great: 1.4733\n",
            "fun: 1.4168\n",
            "well: 1.3296\n",
            "quite: 1.2249\n",
            "life: 1.1709\n",
            "is: 1.1249\n",
            "seen: 1.1234\n",
            "also: 1.1016\n",
            "as: 1.0960\n",
            "very: 1.0673\n",
            "see: 1.0463\n",
            "many: 1.0233\n",
            "most: 1.0108\n",
            "excellent: 1.0045\n",
            "job: 0.9973\n",
            "hilarious: 0.9673\n",
            "terrific: 0.9649\n",
            "truman: 0.9369\n",
            "memorable: 0.9352\n",
            "perfectly: 0.9286\n",
            "american: 0.9233\n",
            "true: 0.9173\n",
            "overall: 0.9031\n",
            "definitely: 0.8988\n",
            "pulp: 0.8985\n",
            "trek: 0.8981\n",
            "especially: 0.8858\n",
            "matrix: 0.8675\n",
            "mulan: 0.8665\n",
            "sometimes: 0.8576\n",
            "rocky: 0.8533\n",
            "will: 0.8438\n",
            "performances: 0.8403\n",
            "perfect: 0.8247\n",
            "he: 0.8210\n",
            "different: 0.8119\n",
            "back: 0.8106\n",
            "best: 0.8104\n",
            "cameron: 0.8057\n",
            "others: 0.7926\n",
            "movies: 0.7922\n",
            "war: 0.7906\n",
            "bowfinger: 0.7736\n",
            "family: 0.7680\n",
            "enjoyed: 0.7639\n",
            "beavis: 0.7608\n",
            "enjoyable: 0.7560\n",
            "you: 0.7544\n",
            "yet: 0.7396\n",
            "Top Negative Features:\n",
            "saved: -0.6404\n",
            "unfunny: -0.6406\n",
            "hurlyburly: -0.6419\n",
            "joke: -0.6462\n",
            "actors: -0.6498\n",
            "dialogue: -0.6600\n",
            "annoying: -0.6671\n",
            "weak: -0.6711\n",
            "clich: -0.6712\n",
            "filmmakers: -0.6745\n",
            "laughable: -0.6758\n",
            "movie: -0.6788\n",
            "flat: -0.6809\n",
            "dutch: -0.6829\n",
            "off: -0.6838\n",
            "bland: -0.6865\n",
            "superior: -0.6875\n",
            "be: -0.6906\n",
            "eve: -0.6911\n",
            "women: -0.6955\n",
            "obvious: -0.6960\n",
            "badly: -0.6970\n",
            "joan: -0.7004\n",
            "so: -0.7028\n",
            "been: -0.7065\n",
            "predictable: -0.7095\n",
            "8mm: -0.7109\n",
            "francis: -0.7121\n",
            "do: -0.7130\n",
            "metro: -0.7182\n",
            "talent: -0.7185\n",
            "guess: -0.7270\n",
            "seagal: -0.7274\n",
            "better: -0.7291\n",
            "else: -0.7304\n",
            "audience: -0.7382\n",
            "disappointing: -0.7442\n",
            "tries: -0.7484\n",
            "worse: -0.7485\n",
            "point: -0.7485\n",
            "least: -0.7540\n",
            "tedious: -0.7703\n",
            "enough: -0.7745\n",
            "adam: -0.7770\n",
            "potential: -0.7791\n",
            "material: -0.7901\n",
            "none: -0.8005\n",
            "pointless: -0.8121\n",
            "fails: -0.8197\n",
            "falls: -0.8243\n",
            "maybe: -0.8244\n",
            "attempts: -0.8377\n",
            "minute: -0.8392\n",
            "given: -0.8412\n",
            "write: -0.8477\n",
            "on: -0.8633\n",
            "anyway: -0.8725\n",
            "poorly: -0.8749\n",
            "tv: -0.8927\n",
            "if: -0.8987\n",
            "west: -0.9044\n",
            "cheap: -0.9049\n",
            "jakob: -0.9162\n",
            "carpenter: -0.9241\n",
            "should: -0.9309\n",
            "director: -0.9432\n",
            "wasted: -0.9669\n",
            "to: -0.9680\n",
            "attempt: -1.0259\n",
            "this: -1.0457\n",
            "terrible: -1.0477\n",
            "lame: -1.0540\n",
            "could: -1.0597\n",
            "mess: -1.0627\n",
            "reason: -1.0641\n",
            "then: -1.0648\n",
            "waste: -1.0743\n",
            "stupid: -1.0750\n",
            "dull: -1.0839\n",
            "here: -1.1004\n",
            "ridiculous: -1.1219\n",
            "harry: -1.1233\n",
            "why: -1.1602\n",
            "even: -1.1736\n",
            "poor: -1.1865\n",
            "looks: -1.1934\n",
            "there: -1.2023\n",
            "awful: -1.2224\n",
            "no: -1.2381\n",
            "any: -1.2610\n",
            "supposed: -1.3291\n",
            "have: -1.3548\n",
            "script: -1.3757\n",
            "boring: -1.4265\n",
            "unfortunately: -1.4911\n",
            "nothing: -1.5273\n",
            "only: -1.6733\n",
            "worst: -1.7620\n",
            "plot: -1.7650\n",
            "bad: -2.8524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STFmeuJ-lkCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "text_classification_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "94px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}